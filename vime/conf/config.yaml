encoder:
  lr: 0.01
  hidden_size: 128
  encoder_layers: 2
  pretext_layers: 2
  p_mask: 0.33
  alpha: 0.5

dataloader:
  batch_size: 128
  n_workers: 10
  train:
    batch_size: ${dataloader.batch_size}
    shuffle: False
    num_workers: ${dataloader.n_workers}
  test:
    batch_size: ${dataloader.batch_size}
    shuffle: False
    num_workers: ${dataloader.n_workers}

optimizer:
  encoder:
    _target_: torch.optim.RMSprop
    _partial_: true

trainer:
  epochs: 10
  accelerator: mps
  devices: 1
